# Hard

You are given an integer array $nums$.

Partition the array into three (possibly empty) subsequences A, B, and C such that every element of $nums$ belongs to exactly one subsequence.

Your goal is to maximize the value of: XOR(A) + AND(B) + XOR(C)

where:

- XOR(arr) denotes the bitwise XOR of all elements in $arr$. If $arr$ is empty, its value is defined as $0$.
- AND(arr) denotes the bitwise AND of all elements in $arr$. If $arr$ is empty, its value is defined as $0$.

Return the maximum value achievable.

```cpp
class Solution {
public:
    // Compute the maximum possible value of: AND(S) + XOR(S') + XOR(S')
    // where S and S' are disjoint subsets of nums and together cover all elements
    long long maximizeXorAndXor(vector<int>& nums) {
        int n = nums.size();
        int totalSubsets = 1 << n;  // Total subsets = 2^n

        // Step 1: Precompute XOR values for all subsets
        vector<int> subsetXor(totalSubsets);
        for (int mask = 1; mask < totalSubsets; ++mask) {
            int lsb = __builtin_ctz(mask);  // index of least significant bit
            subsetXor[mask] = subsetXor[mask ^ (1 << lsb)] ^ nums[lsb];
        }

        // Step 2: Precompute AND values for all subsets
        vector<long long> subsetAnd(totalSubsets);
        subsetAnd[0] = 0; // AND of empty set is conventionally 0 here
        for (int mask = 1; mask < totalSubsets; ++mask) {
            int lsb = __builtin_ctz(mask);
            int reducedMask = mask ^ (1 << lsb);
            subsetAnd[mask] = (reducedMask == 0)
                ? nums[lsb]                       // First element in the subset
                : (subsetAnd[reducedMask] & nums[lsb]); // Combine AND from previous
        }

        const long long FULL_MASK = (1LL << 31) - 1;
        vector<long long> bestSubsetValue(totalSubsets);

        // Step 3: For each subset, compute the best value of XOR(S') + XOR(S') using XOR basis
        for (int mask = 0; mask < totalSubsets; ++mask) {
            long long xorTotal = subsetXor[mask];
            long long zeroBitsMask = ~xorTotal & FULL_MASK;
            long long opt = 0;

            // Gaussian Elimination over GF(2) using XOR basis
            long long xorBasis[31] = {};
            for (int i = 0; i < n; ++i) {
                if ((mask >> i) & 1) {
                    long long reduced = nums[i] & zeroBitsMask;
                    for (int k = 30; k >= 0; --k) {
                        if (!(reduced >> k & 1)) continue;
                        if (!xorBasis[k]) {
                            xorBasis[k] = reduced;
                            break;
                        }
                        reduced ^= xorBasis[k];
                    }
                }
            }

            // Use the basis to maximize the XOR value
            for (int k = 30; k >= 0; --k)
                if ((opt ^ xorBasis[k]) > opt)
                    opt ^= xorBasis[k];

            bestSubsetValue[mask] = xorTotal + (opt << 1);
        }

        // Step 4: Try all partitions of the set into two disjoint subsets: A and B
        // A ∩ B = ∅ and A ∪ B = full set. Let B be the complement of A.
        long long maxResult = 0;
        for (int mask = 0; mask < totalSubsets; ++mask) {
            int complement = ((1 << n) - 1) ^ mask; // All bits not in mask
            maxResult = max(maxResult, subsetAnd[mask] + bestSubsetValue[complement]);
        }

        return maxResult;
    }
};
```
